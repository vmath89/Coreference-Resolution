{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import spacy\n",
    "import numpy as np\n",
    "import json\n",
    "import difflib\n",
    "import math\n",
    "from num2words import num2words\n",
    "#sys.path.insert(0,'/home/vishesh/TUM/Thesis/huggingface/neuralcoref') \n",
    "#from neuralcoref import Coref\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from numpy import array\n",
    "import string \n",
    "import gensim\n",
    "import time\n",
    "import os\n",
    "word2vec = gensim.models.Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GLOVE_DIR = \"/glove6B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format(join(GLOVE_DIR, 'glove.6B.50d.w2vformat.txt'), binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_file_to_list(file):\n",
    "    '''\n",
    "    Convert the training file to a list of strings.\n",
    "    '''\n",
    "    train_list = []\n",
    "    for line in file:\n",
    "        train_list.append(line)\n",
    "    return train_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_documents(train_file):\n",
    "    '''\n",
    "    This function returns a list, where each element of the list is a document from the training file.\n",
    "    '''\n",
    "    train_list = train_file_to_list(train_file)\n",
    "    document = []\n",
    "    part = []\n",
    "    sentence = ''\n",
    "    for i in range (len(train_list)):\n",
    "        if train_list[i] == '\\n':\n",
    "            part.append(sentence)\n",
    "            sentence = ''\n",
    "            continue\n",
    "        cols = train_list[i].split()\n",
    "        if cols[0] == '#begin' or cols[0] == '#end':\n",
    "            if len(part) > 0:\n",
    "                document.append(part)\n",
    "                part = []\n",
    "            continue\n",
    "        else:\n",
    "            if cols[3] == '\\'s' or cols[3] == '.' or cols[3] == ',' or cols[3] == '?':\n",
    "                sentence = sentence.strip() + cols[3] + ' '\n",
    "            else:\n",
    "                sentence += cols[3] + ' '    \n",
    "    return document             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_mention_cluster_list(cluster_start, start_pos, cluster_end, end_pos):\n",
    "    \n",
    "    cluster_start_end_list = []\n",
    "    for start, pos in zip(cluster_start, start_pos):\n",
    "        cluster = [start, pos]\n",
    "        for i in range(len(cluster_end)):\n",
    "            if cluster_end[i] == start:\n",
    "                cluster.append(end_pos[i])\n",
    "                break\n",
    "        del cluster_end[i]\n",
    "        del end_pos[i]\n",
    "        cluster_start_end_list.append(cluster)\n",
    "    return cluster_start_end_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mention(train_list):\n",
    "    '''\n",
    "    From the training list, get the start and end position of the mentions in the documents.\n",
    "    '''\n",
    "    cluster_start = []\n",
    "    start_pos = []\n",
    "    cluster_end = []\n",
    "    end_pos = []\n",
    "    i = 1\n",
    "    for line in train_list:\n",
    "        if line == '\\n' or line == '-':\n",
    "            i += 1\n",
    "            continue\n",
    "        part_number = line.split()[1]\n",
    "        coref_col = line.split()[-1]\n",
    "        for j in range (len(coref_col)):\n",
    "            if coref_col[j] == '(':\n",
    "                cluster_start.append((str(part_number) + '_' + re.findall(r'\\d+', coref_col[j+1:])[0]))\n",
    "                start_pos.append(i)\n",
    "            if coref_col[j] == ')':\n",
    "                cluster_end.append((str(part_number)+ '_' + re.findall(r'\\d+', coref_col[:j])[-1]))\n",
    "                end_pos.append(i)            \n",
    "        i += 1\n",
    "    return cluster_start, start_pos, cluster_end, end_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mention_words(train_file_as_list, pos1, pos2):\n",
    "    '''\n",
    "    Get the words of the mention.\n",
    "    '''\n",
    "    mention = ''\n",
    "    for line_no in range(pos1-1, pos2):\n",
    "        word = train_file_as_list[line_no].split()[3]\n",
    "        #if word == '\\'s' or word == ',' or word == '.':\n",
    "        #    mention = mention.strip() + word + ' '\n",
    "        #else:\n",
    "        mention += word + ' '\n",
    "    return mention.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_preceding_words(list, pos):\n",
    "    '''\n",
    "    Get the 5 preceding words from the starting position of the mention.\n",
    "    '''\n",
    "    word_part = list[pos-1].split()[1]\n",
    "    i = 2\n",
    "    num_words = 0\n",
    "    word = []\n",
    "    while(True):\n",
    "        if list[pos-i] != '\\n':\n",
    "            if list[pos-i].split()[0] == '#begin' or list[pos-i].split()[0] == '#end':\n",
    "                break\n",
    "            part_no = list[pos-i].split()[1]\n",
    "            if part_no == word_part:\n",
    "                word.append(list[pos-i].split()[3])\n",
    "                num_words += 1\n",
    "            if num_words == 5:\n",
    "                break\n",
    "        i += 1\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_next_words(list, pos):\n",
    "    '''\n",
    "    Get 5 words after the last word of the mention.\n",
    "    '''\n",
    "    pos = pos-1\n",
    "    word_part = list[pos].split()[1]\n",
    "    i = 1\n",
    "    num_words = 0\n",
    "    word = []\n",
    "    while(True):\n",
    "        if list[pos+i] != '\\n':\n",
    "            if list[pos+i].split()[0] == '#begin' or list[pos+i].split()[0] == '#end':\n",
    "                break\n",
    "            part_no = list[pos+i].split()[1]\n",
    "            if part_no == word_part:\n",
    "                word.append(list[pos+i].split()[3])\n",
    "                num_words += 1\n",
    "            if num_words == 5:\n",
    "                break\n",
    "        i += 1\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mention_sentence(train_list, pos):\n",
    "    '''\n",
    "    Get the sentence in which the mention occurs.\n",
    "    '''\n",
    "    pos = pos-1\n",
    "    i = 1\n",
    "    start = 0\n",
    "    end = 0\n",
    "    while(True):\n",
    "        if train_list[pos-i] == '\\n':\n",
    "            start = pos-i\n",
    "            break\n",
    "        if train_list[pos-i].split()[0] == '#begin':\n",
    "            start = pos-i\n",
    "            break\n",
    "        i += 1\n",
    "    start += 2\n",
    "    i = 1\n",
    "    while(True):\n",
    "        if train_list[pos+i] == '\\n':\n",
    "            end = pos+i\n",
    "            break\n",
    "        i += 1\n",
    "    sentence = get_mention_words(train_list, start, end)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def document_dictionary(train_file):\n",
    "    documents = get_documents(train_file)\n",
    "    doc_sent = ''\n",
    "    doc_no = 0\n",
    "    doc_dict = {}\n",
    "    for document in documents:\n",
    "        for part in document:\n",
    "            doc_sent += part\n",
    "        doc_dict[doc_no] = doc_sent\n",
    "        doc_sent = ''\n",
    "        doc_no += 1\n",
    "    return doc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mention_length(mention):\n",
    "    '''\n",
    "    The length of the mention\n",
    "    '''\n",
    "    mention_words = mention.split()\n",
    "    mention_len = len(mention_words)\n",
    "    len_in_words = num2words(mention_len)\n",
    "    return len_in_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pronoun: [1, 0, 0, 0]\n",
    "# proper:  [0, 1, 0, 0]\n",
    "# nominal(common noun): [0, 0, 1, 0]\n",
    "# list:    [0, 0, 0, 1]\n",
    "def mention_type(doc, mention):\n",
    "    # pos 0: pronoun, pos 1: proper noun, pos 2: common noun\n",
    "    token_type = [0, 0, 0]\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'PRON':\n",
    "            token_type[0] += 1\n",
    "        elif token.pos_ == 'PROPN':\n",
    "            token_type[1] += 1\n",
    "        elif token.pos_ == 'NOUN':\n",
    "            token_type[2] += 1   \n",
    "    m = max(token_type)\n",
    "    a = [i for i, j in enumerate(token_type) if j == m]  \n",
    "    is_dominant = m >= len(mention.split())/2 \n",
    "    if is_dominant:\n",
    "        if a[0] == 0:\n",
    "            return np.array([1, 0, 0, 0])\n",
    "        if a[0] == 1:\n",
    "            return np.array([0, 1, 0, 0])\n",
    "        if a[0] == 2:\n",
    "            return np.array([0, 0, 1, 0])\n",
    "    else:\n",
    "        return np.array([0, 0, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_mention_contain(newlist):\n",
    "    '''\n",
    "    Check if there is an overlab in the mentions of if is contained fully inside another mention.\n",
    "    '''\n",
    "    for i in range(0, len(newlist)):\n",
    "        start = newlist[i]['mention_start']\n",
    "        end = newlist[i]['mention_end']\n",
    "        for j in range(0, len(newlist)):\n",
    "            c_start = newlist[j]['mention_start']\n",
    "            c_end = newlist[j]['mention_end']\n",
    "            if c_start == start and c_end == end:\n",
    "                continue\n",
    "            if c_start >= start and c_end <= end:\n",
    "                newlist[j]['contained'] = newlist[i]['id']\n",
    "            if c_start >= start and c_start <= end:\n",
    "                newlist[j]['overlap'] = newlist[i]['id']\n",
    "\n",
    "    for k in range(0, len(newlist)):\n",
    "        if 'contained' in newlist[k]:\n",
    "            continue\n",
    "        else:\n",
    "            newlist[k]['contained'] = False\n",
    "        if 'overlap' in newlist[k]:\n",
    "            continue\n",
    "        else:\n",
    "            newlist[k]['overlap'] = False\n",
    "    return newlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_with_N_digits(n):\n",
    "    range_start = 10**(n-1)\n",
    "    range_end = (10**n)-1\n",
    "    return randint(range_start, range_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# not used\n",
    "def get_all_mentions(train_file):\n",
    "    documents = get_documents(train_file)\n",
    "    each_doc = ''\n",
    "    mention_list = []\n",
    "    for docs in documents:\n",
    "        for d in docs:\n",
    "            each_doc += d\n",
    "        print (each_doc)\n",
    "        clusters = coref.one_shot_coref(utterances=each_doc)\n",
    "        mention_list.append(coref.get_mentions())\n",
    "        each_doc = ''\n",
    "    return mention_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#not used\n",
    "def get_all_mention_cluster(file_path, train_file):\n",
    "    mention_list = get_all_mentions(file_path)\n",
    "    train_list = train_file_to_list(train_file)\n",
    "    start_index = []\n",
    "    end_index = []\n",
    "    mention_cluster = []\n",
    "    for doc_num in range(0, len(mention_list)):\n",
    "        for men in mention_list[doc_num]:\n",
    "            for i in range(0, len(train_list)):\n",
    "                if train_list[i] != '\\n' and train_list[i].split()[0] != '#begin' and train_list[i].split()[0] != '#end':\n",
    "                    if train_list[i].split()[3] == str(men[0]) and train_list[i].split()[1] == str(doc_num):\n",
    "                        len_mention = len(men)\n",
    "                        flag = True\n",
    "                        for j, k in zip(men, train_list[i:i+len_mention]):\n",
    "                            if k != '\\n' and k.split()[0] != '#begin' and k.split()[0] != '#end':\n",
    "                                if str(j) != k.split()[3]:\n",
    "                                    flag = False\n",
    "                        start = i+1\n",
    "                        end = i+len_mention\n",
    "                        for s, e in zip(start_index, end_index):\n",
    "                            if s == start and e == end:\n",
    "                                flag = False\n",
    "\n",
    "                        if flag == True:\n",
    "                            start_index.append(start)\n",
    "                            end_index.append(end)\n",
    "                            dummy_list = []\n",
    "                            dummy_list.append(str(doc_num)+'_' + str(random_with_N_digits(10)))\n",
    "                            dummy_list.append(start)\n",
    "                            dummy_list.append(end)\n",
    "                            mention_cluster.append(dummy_list)\n",
    "                            break\n",
    "    return mention_cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_index(mention_info):\n",
    "    '''\n",
    "    The relative position where the mention occurs in the document.\n",
    "    '''\n",
    "    doc_count = '0'\n",
    "    count = 0\n",
    "    i = 0\n",
    "    mentions_in_each_doc = []\n",
    "    for m in mention_info:\n",
    "        if m['id'].split('_')[0] == doc_count:\n",
    "            count += 1\n",
    "        else:\n",
    "            mentions_in_each_doc.append(count)\n",
    "            doc_count = m['id'].split('_')[0]\n",
    "            count = 1\n",
    "        m['index'] = count\n",
    "    mentions_in_each_doc.append(count)\n",
    "    doc_count = '0'\n",
    "    for m in mention_info:\n",
    "        if m['id'].split('_')[0] == doc_count:\n",
    "            m['mention_position'] = m['index']/mentions_in_each_doc[i]\n",
    "        else:\n",
    "            doc_count = m['id'].split('_')[0]\n",
    "            i += 1\n",
    "            m['mention_position'] = m['index']/mentions_in_each_doc[i]\n",
    "            \n",
    "    return mention_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_dictionary(train_file):\n",
    "    '''\n",
    "    Create a dictionary with all the linguistic features of the mentions.\n",
    "    '''\n",
    "    mention_info = []\n",
    "    train_list = train_file_to_list(train_file)\n",
    "    cluster_start, start_pos, cluster_end, end_pos = get_mention(train_list)\n",
    "    mention_cluster = create_mention_cluster_list(cluster_start, start_pos, cluster_end, end_pos)\n",
    "    for m in mention_cluster:\n",
    "        mention_dict = {}\n",
    "        mention_words = get_mention_words(train_list, m[1], m[2])\n",
    "        doc = nlp(mention_words)\n",
    "        mention_dict['id'] = m[0]\n",
    "        mention_dict['mention_start'] = m[1]\n",
    "        mention_dict['mention_end'] = m[2]\n",
    "        mention_dict['mention'] = mention_words\n",
    "        mention_dict['first_word'] = mention_words.split()[0]\n",
    "        mention_dict['last_word'] = mention_words.split()[-1]\n",
    "        if mention_words.isdigit() or mention_words == 'its' or mention_words.lower() == 'that' or mention_words.lower() == 'this':\n",
    "            mention_dict['head_word'] = ''\n",
    "        else:\n",
    "            if len(list(doc.noun_chunks)) > 0:\n",
    "                mention_dict['head_word'] = list(doc.noun_chunks)[0].root.head.text\n",
    "            else:\n",
    "                mention_dict['head_word'] = ''                        \n",
    "        mention_dict['pre_words'] = get_preceding_words(train_list, m[1])\n",
    "        mention_dict['next_words'] = get_next_words(train_list, m[2])\n",
    "        mention_dict['mention_sentence'] = mention_sentence(train_list, m[1])\n",
    "        mention_dict['mention_type'] = mention_type(doc, mention_words).tolist()\n",
    "        mention_dict['mention_length'] = get_mention_length(mention_words)\n",
    "        mention_dict['speaker'] = train_list[m[1] - 1].split()[9]\n",
    "        mention_info.append(mention_dict)\n",
    "    \n",
    "    mention_info = sorted(mention_info, key=lambda k: k['mention_start'])\n",
    "    mention_info = check_mention_contain(mention_info)\n",
    "    mention_info = get_index(mention_info)\n",
    "    return mention_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distance(a):\n",
    "    '''\n",
    "    Convert the distance into a one-hot vector\n",
    "    '''\n",
    "    d = np.zeros((10))\n",
    "    d[a == 0, 0] = 1\n",
    "    d[a == 1, 1] = 1\n",
    "    d[a == 2, 2] = 1\n",
    "    d[a == 3, 3] = 1\n",
    "    d[a == 4, 4] = 1\n",
    "    d[(5 <= a) & (a < 8), 5] = 1\n",
    "    d[(8 <= a) & (a < 16), 6] = 1\n",
    "    d[(16 <= a) & (a < 32), 7] = 1\n",
    "    d[(a >= 32) & (a < 64), 8] = 1\n",
    "    d[a >= 64, 9] = 1\n",
    "    return d.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mention_pairs(train_file):\n",
    "    '''\n",
    "    Creating mention and antecedent pairs\n",
    "    '''\n",
    "    mention_info = train_dictionary(train_file)\n",
    "    mention_pair_list = []\n",
    "    for i in range(1, len(mention_info)):\n",
    "        for j in range(0, i):\n",
    "            pair = []\n",
    "            if mention_info[i]['id'].split('_')[0] == mention_info[j]['id'].split('_')[0]:\n",
    "                pair.append(mention_info[i])\n",
    "                pair.append(mention_info[j])\n",
    "                if mention_info[i]['id'] == mention_info[j]['id']:\n",
    "                    pair.append({'coref': 1})\n",
    "                else:\n",
    "                    if j % 2 == 0 or j % 3 == 0 or j % 5 == 0 or j % 7 == 0 or j % 11 == 0:\n",
    "                        continue\n",
    "                    else:\n",
    "                        pair.append({'coref': 0})\n",
    "                mention_pair_list.append(pair)\n",
    "                \n",
    "    mention_pair_list = get_sentence_dist(mention_pair_list, train_file)\n",
    "    \n",
    "    return mention_pair_list        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentence_dist(mention_pair_list, train_file):\n",
    "    train_list = train_file_to_list(train_file)\n",
    "    for m in mention_pair_list:\n",
    "        count = 0\n",
    "        m1 = m[0]['mention_start']\n",
    "        m2 = m[1]['mention_start']\n",
    "        if m1 < m2:\n",
    "            for t in range(m1, m2+1):\n",
    "                if train_list[t] == '\\n':\n",
    "                    count += 1\n",
    "        seq=difflib.SequenceMatcher(None, m[0]['mention'],m[1]['mention'])\n",
    "        score = seq.ratio()\n",
    "        m.append({'sentence_dist_count': distance(count)})\n",
    "        m.append({'mention_dist_count': distance(m[0]['index'] - m[1]['index'])})\n",
    "        if m[1]['overlap'] == m[0]['id']:\n",
    "            m.append({'overlap': 1})\n",
    "        else:\n",
    "            m.append({'overlap': 0})\n",
    "        if m[1]['speaker'] == m[0]['speaker']:\n",
    "            m.append({'speaker': 1})\n",
    "        else:\n",
    "            m.append({'speaker': 0})\n",
    "        if m[1]['head_word'] == m[0]['head_word']:\n",
    "            m.append({'head_match': 1})\n",
    "        else:\n",
    "            m.append({'head_match': 0})\n",
    "        if m[1]['mention'] == m[0]['mention']:\n",
    "            m.append({'mention_exact_match': 1})\n",
    "        else:\n",
    "            m.append({'mention_exact_match': 0})\n",
    "        if score > 0.6:\n",
    "            m.append({'mention_partial_match': 1})\n",
    "        else:\n",
    "            m.append({'mention_partial_match': 0})\n",
    "    return mention_pair_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_file = open('/cctv_0001.gold_conll', 'r')\n",
    "pairs = get_mention_pairs(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with open('/home/vishesh/TUM/Thesis/coref-json/trainfile1.json', 'w') as outfile:\n",
    "#    json.dump(pairs, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with open('/home/vishesh/TUM/Thesis/coref-json/documents.json', 'w') as outfile:\n",
    "#    json.dump(doc_dict, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Create input vector using the dictionary created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(word):\n",
    "    '''\n",
    "    Convert the word into a 50-dim GloVe vector. If the word does not exist in the GloVe embedding, then \n",
    "    return a vector of zeros.\n",
    "    '''\n",
    "    table = str.maketrans({key: None for key in string.punctuation})\n",
    "    word = word.lower()\n",
    "    if len(word) > 1:\n",
    "        word = word.translate(table)\n",
    "    try:\n",
    "        vec = model[word]\n",
    "    except:\n",
    "        vec = np.zeros((50, 1))\n",
    "    return vec.reshape((50, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_average_vector(word_list):\n",
    "    '''\n",
    "    Return the average of the word embeddings of the list of words.\n",
    "    '''\n",
    "    sum = np.zeros((50, 1))\n",
    "    for i in range(0, len(word_list)):\n",
    "        sum += get_vector(word_list[i])\n",
    "    average_vector = sum/(i+1)\n",
    "    return average_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_docs_average(doc_dict):\n",
    "    '''\n",
    "    Return the average of the word embedding of the entire document.\n",
    "    '''\n",
    "    doc_avg = []\n",
    "    for d in doc_dict:\n",
    "        doc_avg.append(get_average_vector(doc_dict[d].split()))\n",
    "    return doc_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pair_features(feature_list):\n",
    "    '''\n",
    "    Get the distance feature, speaker feature and string matching feature in the form of a vector.\n",
    "    '''\n",
    "    \n",
    "    # distance features\n",
    "    mention_dist = np.array(feature_list[4]['mention_dist_count']).reshape((10, 1))\n",
    "    s_dist = np.array(feature_list[3]['sentence_dist_count']).reshape((10, 1))\n",
    "    overlap = np.array(feature_list[5]['overlap']).reshape((1, 1))\n",
    "    \n",
    "    # speaker feature\n",
    "    speaker = np.array(feature_list[6]['speaker']).reshape((1, 1))\n",
    "    \n",
    "    # string matching features\n",
    "    head_match = np.array(feature_list[7]['head_match']).reshape((1, 1))\n",
    "    mention_exact_match = np.array(feature_list[8]['mention_exact_match']).reshape((1, 1))\n",
    "    mention_partial_match = np.array(feature_list[9]['mention_partial_match']).reshape((1, 1))\n",
    "    \n",
    "    pair_features = np.concatenate((mention_dist, s_dist, overlap, speaker, head_match, \\\n",
    "                                   mention_exact_match, mention_partial_match))\n",
    "    \n",
    "    return pair_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# p: previous, n: next, w: words, a: average, s: sentence\n",
    "def get_mention_features(mention, doc_average):\n",
    "    '''\n",
    "    Get the vector form of all the features extracted for the mentions.\n",
    "    '''\n",
    "    features = []\n",
    "    #head_w = get_vector(mention['head_word'])\n",
    "    first_w = get_vector(mention['first_word'])\n",
    "    last_w = get_vector(mention['last_word'])\n",
    "    mention_length = get_vector(mention['mention_length'])\n",
    "    mention_type = np.array(mention['mention_type']).reshape((4, 1))\n",
    "    mention_position = np.array(mention['mention_position']).reshape((1, 1))\n",
    "    if mention['contained'] == False:\n",
    "        mention_contain = np.zeros((1, 1))\n",
    "    else:\n",
    "        mention_contain = np.ones((1, 1))\n",
    "    if len(mention['pre_words']) > 0:\n",
    "        mention_p_w1 = get_vector(mention['pre_words'][0])\n",
    "    else:\n",
    "        mention_p_w1 = np.zeros((50, 1))\n",
    "    if len(mention['pre_words']) > 1:\n",
    "        mention_p_w2 = get_vector(mention['pre_words'][1])\n",
    "    else:\n",
    "        mention_p_w2 = np.zeros((50, 1))\n",
    "    if len(mention['next_words']) > 0:\n",
    "        mention_n_w1 = get_vector(mention['next_words'][0])\n",
    "    else:\n",
    "        mention_n_w1 = np.zeros((50, 1))\n",
    "    if len(mention['next_words']) > 1:\n",
    "        mention_n_w2 = get_vector(mention['next_words'][1])\n",
    "    else:\n",
    "        mention_n_w2 = np.zeros((50, 1))\n",
    "    if len(mention['pre_words']) > 0:\n",
    "        mention_p_w_a = get_average_vector(mention['pre_words'])\n",
    "    else:\n",
    "        mention_p_w_a = np.zeros((50, 1))\n",
    "    if len(mention['next_words']) > 0:\n",
    "        mention_n_w_a = get_average_vector(mention['next_words'])\n",
    "    else:\n",
    "        mention_n_w_a = np.zeros((50, 1))\n",
    "    mention_s_a = get_average_vector(mention['mention_sentence'].split())\n",
    "    doc_id = mention['id'].split('_')[0]\n",
    "    doc_avg = doc_average[int(doc_id)]\n",
    "    \n",
    "    \n",
    "    features = np.concatenate((first_w, last_w, mention_p_w1, mention_p_w2, mention_p_w_a, \\\n",
    "                               mention_n_w1, mention_n_w2, mention_n_w_a, mention_s_a, mention_length, \\\n",
    "                               mention_type, mention_position, mention_contain, doc_avg))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_feature_input(pairs, doc_dict):\n",
    "    '''\n",
    "    Concatenate the features of the mentions, candidate antecedents and the pair features(antecedent, mention pair)\n",
    "    '''\n",
    "    docs_avg = calculate_docs_average(doc_dict)\n",
    "    input_feature_list = []\n",
    "    i = 0\n",
    "    for m in pairs:\n",
    "        i += 1\n",
    "        input_feature_vector = []\n",
    "        mention_avg = get_average_vector(m[0]['mention'].split())\n",
    "        antecedent_avg = get_average_vector(m[1]['mention'].split())\n",
    "        mention_features = get_mention_features(m[0], docs_avg)\n",
    "        antecedent_features = get_mention_features(m[1], docs_avg)\n",
    "        pair_features = get_pair_features(m)\n",
    "        \n",
    "        input_feature_vector.append(antecedent_avg)\n",
    "        input_feature_vector.append(antecedent_features)\n",
    "        input_feature_vector.append(mention_avg)\n",
    "        input_feature_vector.append(mention_features)\n",
    "        input_feature_vector.append(pair_features)\n",
    "        input_feature_list.append(input_feature_vector)\n",
    "        \n",
    "    return input_feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_input_vector(pairs, doc_dict):\n",
    "    feature_input = make_feature_input(pairs, doc_dict)\n",
    "    len_f_input = len(feature_input)\n",
    "    input_ = []\n",
    "    for f_input in feature_input:\n",
    "        con = np.concatenate((f_input[0], f_input[1], f_input[2], f_input[3], f_input[4]))\n",
    "        input_.append(con)\n",
    "        del con\n",
    "    return input_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_output_vector(pairs):\n",
    "    '''\n",
    "    The vector for the output labels\n",
    "    '''\n",
    "    output = []\n",
    "    len_mentions = len(pairs)\n",
    "    for m in pairs:\n",
    "        output.append(m[2]['coref'])\n",
    "    output = np.array(output).reshape((len_mentions, 1))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_train_files = '/english/annotations/'\n",
    "path_to_dev_files = '/english/annotations/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_file = open(path_to_train_file, 'r')\n",
    "#doc_dict = document_dictionary(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_of_conll_files = []\n",
    "for path, subdirs, files in os.walk(path_to_train_files):\n",
    "    for name in files:\n",
    "        if name.endswith(\".gold_conll\"):\n",
    "            list_of_conll_files.append(os.path.join(path, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_network_data(path):\n",
    "    train_file = open(path, 'r')\n",
    "    doc_dict = document_dictionary(train_file)\n",
    "    train_file = open(path, 'r')\n",
    "    pairs = get_mention_pairs(train_file)\n",
    "    input_vector = make_input_vector(pairs, doc_dict)\n",
    "    output_vector = make_output_vector(pairs)\n",
    "    return input_vector, output_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Run this cell for getting training files\n",
    "FILES = 100\n",
    "num_train_files = len(list_of_conll_files)\n",
    "count = 0\n",
    "num = 0\n",
    "file_num = 1\n",
    "for i in range (0, FILES):\n",
    "    input_files_vector = []\n",
    "    output_files_vector = []\n",
    "    for j in range(math.ceil((i/FILES) * num_train_files), math.ceil(((i+1)/FILES) * num_train_files)):\n",
    "        num += 1\n",
    "        print ('file num: ' + str(num))\n",
    "        i_vector, o_vector = train_network_data(list_of_conll_files[j])\n",
    "        if len(i_vector) > 0:\n",
    "            input_files_vector.append(i_vector)\n",
    "            output_files_vector.append(o_vector)\n",
    "            count += 1\n",
    "            print ('coref: ' + str(count))\n",
    "            if count % 5 == 0:\n",
    "                ffnn_input = []\n",
    "                ffnn_output = []\n",
    "                for inp_vector, out_vector in zip(input_files_vector, output_files_vector):\n",
    "                    for inp, out in zip(inp_vector, out_vector):\n",
    "                        ffnn_input.append(inp)\n",
    "                        ffnn_output.append(out)\n",
    "                np.save('/home/vishesh/TUM/Thesis/Coreference-Resolution/data/processed/new/ffnn_train_' + str(file_num), ffnn_input, allow_pickle=True, fix_imports=True)    \n",
    "                np.save('/home/vishesh/TUM/Thesis/Coreference-Resolution/data/processed/new/ffnn_labels_' + str(file_num), ffnn_output, allow_pickle=True, fix_imports=True)\n",
    "                print ('File  created.')\n",
    "                file_num += 1\n",
    "    if count == 100:\n",
    "        break\n",
    "np.save('/home/vishesh/TUM/Thesis/Coreference-Resolution/data/processed/new/ffnn_train_' + str(i), ffnn_input, allow_pickle=True, fix_imports=True)    \n",
    "np.save('/home/vishesh/TUM/Thesis/Coreference-Resolution/data/processed/new/ffnn_labels_' + str(i), ffnn_output, allow_pickle=True, fix_imports=True)\n",
    "print ('File  created.')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run this cell for getting the development files.\n",
    "input_files_vector = []\n",
    "output_files_vector = []\n",
    "num = 0\n",
    "count = 0\n",
    "for j in range(0, len(list_of_conll_files)):\n",
    "    num += 1\n",
    "    print ('file num: ' + str(num))\n",
    "    i_vector, o_vector = train_network_data(list_of_conll_files[j])\n",
    "    if len(i_vector) > 0:\n",
    "        input_files_vector.append(i_vector)\n",
    "        output_files_vector.append(o_vector)\n",
    "        count += 1\n",
    "        print ('coref: ' + str(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ffnn_input = []\n",
    "ffnn_output = []\n",
    "for inp_vector, out_vector in zip(input_files_vector, output_files_vector):\n",
    "    for inp, out in zip(inp_vector, out_vector):\n",
    "        ffnn_input.append(inp)\n",
    "        ffnn_output.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('/home/vishesh/TUM/Thesis/Coreference-Resolution/data/processed/ffnn_input_dev', ffnn_input, allow_pickle=True, fix_imports=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('/home/vishesh/TUM/Thesis/Coreference-Resolution/data/processed/ffnn_output_dev', ffnn_output, allow_pickle=True, fix_imports=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
