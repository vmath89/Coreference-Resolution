{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import spacy\n",
    "import math\n",
    "import difflib\n",
    "from spacy.symbols import nsubj, VERB\n",
    "from num2words import num2words\n",
    "import gensim\n",
    "import string\n",
    "word2vec = gensim.models.Word2Vec\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GLOVE_DIR = '/home/vishesh/TUM/Thesis/glove6B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_model = gensim.models.KeyedVectors.load_word2vec_format(join(GLOVE_DIR, 'glove.6B.50d.w2vformat.txt'), binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = 1237\n",
    "HIDDEN_DIM1 = 1000\n",
    "HIDDEN_DIM2 = 500\n",
    "HIDDEN_DIM3 = 500\n",
    "OUTPUT_DIM = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FFNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, hidden_dim3, output_dim):\n",
    "        super(FFNN, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.fc3 = nn.Linear(hidden_dim2, hidden_dim3)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        self.fc4 = nn.Linear(hidden_dim2, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #x = x.view(-1, 1337)\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu3(out)\n",
    "        out = self.fc4(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = FFNN(INPUT_DIM, HIDDEN_DIM1, HIDDEN_DIM2, HIDDEN_DIM3, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = torch.load('/home/vishesh/TUM/Thesis/Coreference-Resolution/models/bestModel.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dev = np.load('/home/vishesh/TUM/Thesis/Coreference-Resolution/data/processed/ffnn_input_dev.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_dev = np.load('/home/vishesh/TUM/Thesis/Coreference-Resolution/data/processed/ffnn_output_dev.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = (torch.from_numpy(input_dev[9].reshape(INPUT_DIM))).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "o = torch.from_numpy(output_dev[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = model(Variable(i))\n",
    "nnjk, predicted = torch.max(output.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0\n",
       "[torch.LongTensor of size 1]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = 'Standing tall on the Taihang Mountain is the Monument to the Hundred Regiments Offensive.'\n",
    "s2 = 'The child is running very fast.'\n",
    "s3 = 'This map reflected the European battlefield situation.'\n",
    "s4 = 'From one side, it siezed an important city in China called Yichang.'\n",
    "s5 = 'It was this year that the Japanese Army developed the strategy.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mentions(sentence):\n",
    "    '''\n",
    "    This function returns all the possible mentions for any given sentence.\n",
    "    \n",
    "    Args:\n",
    "    sentence: The sentence for which we want to find the mentions.\n",
    "    \n",
    "    Returns:\n",
    "    mentions: list of all the mentions in the sentence.\n",
    "    '''\n",
    "    verbs = []\n",
    "    mentions = []\n",
    "    tokens = nlp(sentence)\n",
    "    for token, np in zip(tokens, tokens.noun_chunks):\n",
    "        if token.pos_ == 'PRON':\n",
    "            mentions.append(token.text)\n",
    "            if token.dep == nsubj and token.head.pos == VERB:\n",
    "                mentions.append(token.head.text)\n",
    "        if np.text not in mentions:\n",
    "            mentions.append(np.text)\n",
    "    return mentions\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It', 'was', 'the Japanese Army', 'the strategy']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_mentions(s5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'/home/vishesh/TUM/Thesis/huggingface/neuralcoref') \n",
    "from neuralcoref import Coref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spacy model\n",
      "\n",
      "    \u001b[93mInfo about model en_core_web_sm\u001b[0m\n",
      "\n",
      "    lang               en             \n",
      "    pipeline           ['tagger', 'parser', 'ner']\n",
      "    accuracy           {'token_acc': 99.8698372794, 'ents_p': 84.9664503965, 'ents_r': 85.6312524451, 'uas': 91.7237657538, 'tags_acc': 97.0403350292, 'ents_f': 85.2975560875, 'las': 89.800872413}\n",
      "    name               core_web_sm    \n",
      "    license            CC BY-SA 3.0   \n",
      "    author             Explosion AI   \n",
      "    url                https://explosion.ai\n",
      "    vectors            {'keys': 0, 'width': 0, 'vectors': 0}\n",
      "    sources            ['OntoNotes 5', 'Common Crawl']\n",
      "    version            2.0.0          \n",
      "    spacy_version      >=2.0.0a18     \n",
      "    parent_package     spacy          \n",
      "    speed              {'gpu': None, 'nwords': 291344, 'cpu': 5122.3040471407}\n",
      "    email              contact@explosion.ai\n",
      "    description        English multi-task CNN trained on OntoNotes, with GloVe vectors trained on Common Crawl. Assigns word vectors, context-specific token vectors, POS tags, dependency parse and named entities.\n",
      "    source             /home/vishesh/anaconda3/lib/python3.6/site-packages/en_core_web_sm\n",
      "\n",
      "loading model from /home/vishesh/TUM/Thesis/huggingface/neuralcoref/neuralcoref/weights/\n"
     ]
    }
   ],
   "source": [
    "coref = Coref()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mention_list = []\n",
    "clusters = coref.one_shot_coref(utterances=s5)\n",
    "mention_list.append(coref.get_mentions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[It, this year, the Japanese Army, the strategy]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mention_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pre_words(passage, mention):\n",
    "    '''\n",
    "    This function returns the previous 5 words of the mention from the passage\n",
    "    \n",
    "    Args:\n",
    "    passage: The passage from which the previous words have to be chosen.\n",
    "    mention: The word whose previous words have to be found.\n",
    "    \n",
    "    Returns:\n",
    "    pre_words: Previous 5 words from the passage with respect to mention\n",
    "    '''\n",
    "\n",
    "    tokens = nlp(passage)\n",
    "    mention_tokens = mention.split()\n",
    "    for i in range(0, len(tokens)):\n",
    "        if str(tokens[i]) == mention_tokens[0]:\n",
    "            flag = True\n",
    "            c = i\n",
    "            for m in range(0, len(mention_tokens)):\n",
    "                if mention.split()[m] != tokens[c].text:\n",
    "                    flag = False\n",
    "                c += 1\n",
    "            if flag == True:\n",
    "                break\n",
    "    pre_words = []\n",
    "    for p in range(i-1, i-6, -1):\n",
    "        pre_words.append(str(tokens[p]))\n",
    "        if p == 0:\n",
    "            break\n",
    "    return pre_words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_next_words(passage, mention):\n",
    "    '''\n",
    "    This function returns the next 5 words of the mention from the passage.\n",
    "    \n",
    "    Args:\n",
    "    passage: The passage from which the next words have to be chosen.\n",
    "    mention: The word whose next words have to be found.\n",
    "    \n",
    "    Returns:\n",
    "    next_words: Next 5 words from the passage with respect to mention.\n",
    "    '''\n",
    "    tokens = nlp(passage)\n",
    "    mention_tokens = mention.split()\n",
    "    for i in range(0, len(tokens)):\n",
    "        if str(tokens[i]) == mention_tokens[0]:\n",
    "            flag = True\n",
    "            for m in range(0, len(mention_tokens)):\n",
    "                if mention.split()[m] != tokens[i].text:\n",
    "                    flag = False\n",
    "                i += 1\n",
    "            if flag == True:\n",
    "                break\n",
    "    next_words = []\n",
    "    for p in range(i, i + 5):\n",
    "        next_words.append(str(tokens[p]))\n",
    "        if p == len(tokens) -1:\n",
    "            break\n",
    "    return next_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mention_sentence(passage, mention):\n",
    "    '''\n",
    "    This function returns the sentence in which the mention occurs.\n",
    "    \n",
    "    Args:\n",
    "    passage: The passage from which the next words have to be chosen.\n",
    "    mention: The word whose next words have to be found\n",
    "    \n",
    "    Returns:\n",
    "    sentence: The sentence in which the mention occurs.\n",
    "    s: Index of the sentence in the passage.\n",
    "    '''\n",
    "    sentences = splitParagraphIntoSentences(passage)\n",
    "    mention_tokens = mention.split()\n",
    "    flag = False\n",
    "    for s in range(0, len(sentences)):\n",
    "        s_tokens = nlp(sentences[s])\n",
    "        for i in range(0, len(s_tokens)):\n",
    "            if str(s_tokens[i]) == mention_tokens[0]:\n",
    "                flag = True\n",
    "                for m in range(0, len(mention_tokens)):\n",
    "                    if mention_tokens[m] != str(s_tokens[i]):\n",
    "                        flag = False\n",
    "                    i += 1\n",
    "                if flag == True:\n",
    "                    break\n",
    "        if flag == True:\n",
    "            break\n",
    "        \n",
    "    return sentences[s], s+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inspired from this post.\n",
    "# https://stackoverflow.com/questions/8465335/a-regex-for-extracting-sentence-from-a-paragraph-in-python\n",
    "def splitParagraphIntoSentences(paragraph):\n",
    "    import re\n",
    "    sentenceEnders = re.compile(r\"\"\"\n",
    "        # Split sentences on whitespace between them.\n",
    "        (?:               # Group for two positive lookbehinds.\n",
    "          (?<=[.!?])      # Either an end of sentence punct,\n",
    "        | (?<=[.!?]['\"])  # or end of sentence punct and quote.\n",
    "        )                 # End group of two positive lookbehinds.\n",
    "        (?<!  Mr\\.   )    # Don't end sentence on \"Mr.\"\n",
    "        (?<!  Mrs\\.  )    # Don't end sentence on \"Mrs.\"\n",
    "        (?<!  Jr\\.   )    # Don't end sentence on \"Jr.\"\n",
    "        (?<!  Dr\\.   )    # Don't end sentence on \"Dr.\"\n",
    "        (?<!  Prof\\. )    # Don't end sentence on \"Prof.\"\n",
    "        (?<!  Sr\\.   )    # Don't end sentence on \"Sr.\"\n",
    "        \\s+               # Split on whitespace between sentences.\n",
    "        \"\"\", \n",
    "        re.IGNORECASE | re.VERBOSE)\n",
    "    sentenceList = sentenceEnders.split(paragraph)\n",
    "    return sentenceList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('It was great.', 2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_mention_sentence('It was this year that the Japanese Army developed the strategy. It was great. I loved it.', 'great')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# pronoun: [1, 0, 0, 0]\n",
    "# proper:  [0, 1, 0, 0]\n",
    "# nominal(common noun): [0, 0, 1, 0]\n",
    "# list:    [0, 0, 0, 1]\n",
    "def mention_type(doc, mention):\n",
    "    # pos 0: pronoun, pos 1: proper noun, pos 2: common noun\n",
    "    '''\n",
    "    This function returns the type of mention in the above annotated form.\n",
    "    '''\n",
    "    token_type = [0, 0, 0]\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'PRON':\n",
    "            token_type[0] += 1\n",
    "        elif token.pos_ == 'PROPN':\n",
    "            token_type[1] += 1\n",
    "        elif token.pos_ == 'NOUN':\n",
    "            token_type[2] += 1   \n",
    "    m = max(token_type)\n",
    "    a = [i for i, j in enumerate(token_type) if j == m]  \n",
    "    is_dominant = m >= len(mention.split())/2 \n",
    "    if is_dominant:\n",
    "        if a[0] == 0:\n",
    "            return np.array([1, 0, 0, 0])\n",
    "        if a[0] == 1:\n",
    "            return np.array([0, 1, 0, 0])\n",
    "        if a[0] == 2:\n",
    "            return np.array([0, 0, 1, 0])\n",
    "    else:\n",
    "        return np.array([0, 0, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mention_position(mentions, m):\n",
    "    '''\n",
    "    This function returns the position of the mention m relative to the all the other mentions.\n",
    "    \n",
    "    Args:\n",
    "    mentions: all the mentions in the passage.\n",
    "    m: the mention whose relative index is to be found.\n",
    "    \n",
    "    Returns:\n",
    "    position: position of mention m relative to all the other mentions.\n",
    "    '''\n",
    "    num_mentions = len(mentions)\n",
    "    for index in range(num_mentions):\n",
    "        if mentions[index] == m:\n",
    "            break\n",
    "    position = num_mentions/(index + 1)\n",
    "    return position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mention_contain(mentions, m):\n",
    "    '''\n",
    "    This function checks whether the mention m is contained inside any other mention or not.\n",
    "    \n",
    "    Args:\n",
    "    mentions: all the mentions in the passage.\n",
    "    m: the mention whose relative index is to be found.\n",
    "    \n",
    "    Returns:\n",
    "    True or False\n",
    "    '''\n",
    "    flag = False\n",
    "    for mention in mentions:\n",
    "        if m in mention:\n",
    "            if m == mention:\n",
    "                continue\n",
    "            flag = True\n",
    "            break\n",
    "    return flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the Taihang Mountain', 'the Monument', 'the Hundred Regiments Offensive']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mentions = get_mentions(sentence)\n",
    "print (mentions)\n",
    "mention_contain(mentions, 'Monumentt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mention_length(mention):\n",
    "    '''\n",
    "    This function returns the length of the mention in words.\n",
    "    \n",
    "    Args:\n",
    "    mention: The mention whose length in words is required.\n",
    "    \n",
    "    Returns:\n",
    "    len_in_words: Length in words like one, two, etc.\n",
    "    '''\n",
    "    mention_words = mention.split()\n",
    "    mention_len = len(mention_words)\n",
    "    len_in_words = num2words(mention_len)\n",
    "    return len_in_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distance(a):\n",
    "    '''\n",
    "    This function takes in an integer and returns a 10 dimentional vector.\n",
    "    \n",
    "    Args: \n",
    "    a(int): The integer whose vector has to be calculated.\n",
    "    \n",
    "    Returns:\n",
    "    list: a 10 dim vector.\n",
    "    '''\n",
    "    d = np.zeros((10))\n",
    "    d[a == 0, 0] = 1\n",
    "    d[a == 1, 1] = 1\n",
    "    d[a == 2, 2] = 1\n",
    "    d[a == 3, 3] = 1\n",
    "    d[a == 4, 4] = 1\n",
    "    d[(5 <= a) & (a < 8), 5] = 1\n",
    "    d[(8 <= a) & (a < 16), 6] = 1\n",
    "    d[(16 <= a) & (a < 32), 7] = 1\n",
    "    d[(a >= 32) & (a < 64), 8] = 1\n",
    "    d[a >= 64, 9] = 1\n",
    "    return d.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(passage):\n",
    "    '''\n",
    "    This function takes in the passage for which the mention and its features have to be found and \n",
    "    returns all the features about each mention in the passage.\n",
    "    \n",
    "    Args:\n",
    "    passage: The passage whose mention and features have to be found.\n",
    "    \n",
    "    Returns:\n",
    "    mention_info: A list with all the mentions along with its features.\n",
    "    '''\n",
    "    mention_info = []\n",
    "    mentions = get_mentions(passage)\n",
    "    count = 1\n",
    "    for m in mentions:\n",
    "        mention_dict = {}\n",
    "        doc = nlp(m)\n",
    "        mention_dict['id'] = count\n",
    "        mention_dict['mention'] = m\n",
    "        mention_dict['first_word'] = str(doc[0])\n",
    "        mention_dict['last_word'] = str(doc[-1])\n",
    "        if m.isdigit() or m == 'its' or m.lower() == 'that' or m.lower() == 'this':\n",
    "            mention_dict['head_word'] = ''\n",
    "        else:\n",
    "            if len(list(doc.noun_chunks)) > 0:\n",
    "                mention_dict['head_word'] = list(doc.noun_chunks)[0].root.head.text\n",
    "            else:\n",
    "                mention_dict['head_word'] = ''\n",
    "        mention_dict['pre_words'] = get_pre_words(passage, m)\n",
    "        mention_dict['next_words'] = get_next_words(passage, m)\n",
    "        sentence, sen_index = get_mention_sentence(passage,m)\n",
    "        mention_dict['mention_sentence'] = sentence\n",
    "        mention_dict['sentence_index'] = sen_index\n",
    "        mention_dict['mention_type'] = mention_type(doc, m)\n",
    "        mention_dict['speaker'] = 'Speaker#1'\n",
    "        mention_dict['mention_position'] = get_mention_position(mentions, m)\n",
    "        mention_dict['contained'] = mention_contain(mentions, m)\n",
    "        mention_dict['mention_length'] = get_mention_length(m)\n",
    "        mention_info.append(mention_dict)\n",
    "        count += 1\n",
    "    return mention_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def overlap(mention1, mention2):\n",
    "    '''\n",
    "    This function checks if mention2 overlaps the mention1 or not.\n",
    "    \n",
    "    Args:\n",
    "    mention1: Mention\n",
    "    mention2: Mention\n",
    "    \n",
    "    Returns:\n",
    "    True or False\n",
    "    '''\n",
    "    flag = 0\n",
    "    if mention2 in mention1:\n",
    "        flag = 1\n",
    "\n",
    "    return flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_pairs(passage):\n",
    "    '''\n",
    "    This function takes in the passage whose mentions have to be found, extracts the mention features \n",
    "    and returns a list with the pair of mentions.\n",
    "    \n",
    "    Args:\n",
    "    passage: The text which has to be parsed for mentions\n",
    "    \n",
    "    Returns:\n",
    "    pairs_list: List of pairs of mentions which have to checked for coreference.\n",
    "    '''\n",
    "    mentions_features = extract_features(passage)\n",
    "    pairs_list = []\n",
    "    num_mentions = len(mentions_features)\n",
    "    for i in range(num_mentions - 1):\n",
    "        for j in range(i+1, num_mentions):\n",
    "            pairs = []\n",
    "            pairs.append(mentions_features[i])\n",
    "            pairs.append(mentions_features[j])\n",
    "            pairs_list.append(pairs)\n",
    "    return pairs_list\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pair_features(passage):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    pairs = make_pairs(passage)\n",
    "    for p in pairs:\n",
    "        \n",
    "        #print (p)\n",
    "        seq=difflib.SequenceMatcher(None, p[0]['mention'],p[1]['mention'])\n",
    "        score = seq.ratio()\n",
    "        p.append({'mention_distance': distance(abs(p[0]['id'] - p[1]['id']))})\n",
    "        p.append({'sentence_distance': distance(abs(p[0]['sentence_index'] - p[1]['sentence_index']))})\n",
    "        p.append({'overlap': overlap(p[0]['mention'], p[1]['mention'])})\n",
    "        p.append({'speaker': 1})\n",
    "        if p[1]['head_word'] == p[0]['head_word']:\n",
    "            p.append({'head_match': 1})\n",
    "        else:\n",
    "            p.append({'head_match': 0})\n",
    "        if p[1]['mention'] == p[0]['mention']:\n",
    "            p.append({'mention_exact_match': 1})\n",
    "        else:\n",
    "            p.append({'mention_exact_match': 0})\n",
    "        if score > 0.6:\n",
    "            p.append({'mention_partial_match': 1})\n",
    "        else:\n",
    "            p.append({'mention_partial_match': 0})\n",
    "        \n",
    "    return pairs        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'contained': False,\n",
       "   'first_word': 'the',\n",
       "   'head_word': 'Mountain',\n",
       "   'id': 1,\n",
       "   'last_word': 'Mountain',\n",
       "   'mention': 'the Taihang Mountain',\n",
       "   'mention_length': 'three',\n",
       "   'mention_position': 3.0,\n",
       "   'mention_sentence': 'Standing tall on the Taihang Mountain is the Monument to the Hundred Regiments Offensive.',\n",
       "   'mention_type': array([0, 1, 0, 0]),\n",
       "   'next_words': ['is', 'the', 'Monument', 'to', 'the'],\n",
       "   'pre_words': ['on', 'tall', 'Standing'],\n",
       "   'sentence_index': 1,\n",
       "   'speaker': 'Speaker#1'},\n",
       "  {'contained': False,\n",
       "   'first_word': 'the',\n",
       "   'head_word': 'Monument',\n",
       "   'id': 2,\n",
       "   'last_word': 'Monument',\n",
       "   'mention': 'the Monument',\n",
       "   'mention_length': 'two',\n",
       "   'mention_position': 1.5,\n",
       "   'mention_sentence': 'Standing tall on the Taihang Mountain is the Monument to the Hundred Regiments Offensive.',\n",
       "   'mention_type': array([0, 1, 0, 0]),\n",
       "   'next_words': ['to', 'the', 'Hundred', 'Regiments', 'Offensive'],\n",
       "   'pre_words': ['is', 'Mountain', 'Taihang', 'the', 'on'],\n",
       "   'sentence_index': 1,\n",
       "   'speaker': 'Speaker#1'},\n",
       "  {'mention_distance': [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]},\n",
       "  {'sentence_distance': [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]},\n",
       "  {'overlap': 0},\n",
       "  {'speaker': 1},\n",
       "  {'head_match': 0},\n",
       "  {'mention_exact_match': 0},\n",
       "  {'mention_partial_match': 0}],\n",
       " [{'contained': False,\n",
       "   'first_word': 'the',\n",
       "   'head_word': 'Mountain',\n",
       "   'id': 1,\n",
       "   'last_word': 'Mountain',\n",
       "   'mention': 'the Taihang Mountain',\n",
       "   'mention_length': 'three',\n",
       "   'mention_position': 3.0,\n",
       "   'mention_sentence': 'Standing tall on the Taihang Mountain is the Monument to the Hundred Regiments Offensive.',\n",
       "   'mention_type': array([0, 1, 0, 0]),\n",
       "   'next_words': ['is', 'the', 'Monument', 'to', 'the'],\n",
       "   'pre_words': ['on', 'tall', 'Standing'],\n",
       "   'sentence_index': 1,\n",
       "   'speaker': 'Speaker#1'},\n",
       "  {'contained': False,\n",
       "   'first_word': 'the',\n",
       "   'head_word': 'Offensive',\n",
       "   'id': 3,\n",
       "   'last_word': 'Offensive',\n",
       "   'mention': 'the Hundred Regiments Offensive',\n",
       "   'mention_length': 'four',\n",
       "   'mention_position': 1.0,\n",
       "   'mention_sentence': 'Standing tall on the Taihang Mountain is the Monument to the Hundred Regiments Offensive.',\n",
       "   'mention_type': array([0, 1, 0, 0]),\n",
       "   'next_words': ['.'],\n",
       "   'pre_words': ['to', 'Monument', 'the', 'is', 'Mountain'],\n",
       "   'sentence_index': 1,\n",
       "   'speaker': 'Speaker#1'},\n",
       "  {'mention_distance': [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]},\n",
       "  {'sentence_distance': [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]},\n",
       "  {'overlap': 0},\n",
       "  {'speaker': 1},\n",
       "  {'head_match': 0},\n",
       "  {'mention_exact_match': 0},\n",
       "  {'mention_partial_match': 0}],\n",
       " [{'contained': False,\n",
       "   'first_word': 'the',\n",
       "   'head_word': 'Monument',\n",
       "   'id': 2,\n",
       "   'last_word': 'Monument',\n",
       "   'mention': 'the Monument',\n",
       "   'mention_length': 'two',\n",
       "   'mention_position': 1.5,\n",
       "   'mention_sentence': 'Standing tall on the Taihang Mountain is the Monument to the Hundred Regiments Offensive.',\n",
       "   'mention_type': array([0, 1, 0, 0]),\n",
       "   'next_words': ['to', 'the', 'Hundred', 'Regiments', 'Offensive'],\n",
       "   'pre_words': ['is', 'Mountain', 'Taihang', 'the', 'on'],\n",
       "   'sentence_index': 1,\n",
       "   'speaker': 'Speaker#1'},\n",
       "  {'contained': False,\n",
       "   'first_word': 'the',\n",
       "   'head_word': 'Offensive',\n",
       "   'id': 3,\n",
       "   'last_word': 'Offensive',\n",
       "   'mention': 'the Hundred Regiments Offensive',\n",
       "   'mention_length': 'four',\n",
       "   'mention_position': 1.0,\n",
       "   'mention_sentence': 'Standing tall on the Taihang Mountain is the Monument to the Hundred Regiments Offensive.',\n",
       "   'mention_type': array([0, 1, 0, 0]),\n",
       "   'next_words': ['.'],\n",
       "   'pre_words': ['to', 'Monument', 'the', 'is', 'Mountain'],\n",
       "   'sentence_index': 1,\n",
       "   'speaker': 'Speaker#1'},\n",
       "  {'mention_distance': [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]},\n",
       "  {'sentence_distance': [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]},\n",
       "  {'overlap': 0},\n",
       "  {'speaker': 1},\n",
       "  {'head_match': 0},\n",
       "  {'mention_exact_match': 0},\n",
       "  {'mention_partial_match': 0}]]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = get_pair_features(sentence)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_vector(word):\n",
    "    '''\n",
    "    This function takes in the word for which we want to find the GLoVe vector and returns the \n",
    "    50 dim representation of the word.\n",
    "    \n",
    "    Args:\n",
    "    word(string): The word for which we want to find the vector.\n",
    "    \n",
    "    Returns:\n",
    "    vector: A 50 dim representation of the word as per GLoVe.    \n",
    "    '''\n",
    "    \n",
    "    table = str.maketrans({key: None for key in string.punctuation})\n",
    "    word = word.lower()\n",
    "    if len(word) > 1:\n",
    "        word = word.translate(table)\n",
    "    try:\n",
    "        vec = glove_model[word]\n",
    "    except:\n",
    "        vec = np.zeros((50, 1))\n",
    "    return vec.reshape((50, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get_vector('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_average_vector(word_list):\n",
    "    '''\n",
    "    This function takes in a list of words and returns the average GLoVe vector for all the words.\n",
    "    \n",
    "    Args:\n",
    "    word_list(list of string): The list of words whose average vector has to be calculated.\n",
    "    \n",
    "    Returns:\n",
    "    average_vector: A 50 dim GLoVe representation of the average of the list of words.\n",
    "    '''\n",
    "    sum = np.zeros((50, 1))\n",
    "    for i in range(0, len(word_list)):\n",
    "        sum += get_vector(word_list[i])\n",
    "    average_vector = sum/(i+1)\n",
    "    return average_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get_vector('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# p: previous, n: next, w: words, a: average, s: sentence\n",
    "def get_mention_features_vector(mention_features, passage):\n",
    "    '''\n",
    "    This function takes in all the features per mention and returns a vector of those mentions features.\n",
    "    \n",
    "    Args:\n",
    "    mention_features: All the features of the mention which has to be converted to a vector.\n",
    "    passage: The passage to which the mention belongs.\n",
    "    \n",
    "    Returns:\n",
    "    features: A vector unilizing all the features of the mention.\n",
    "    '''\n",
    "    first_w = get_vector(mention_features['first_word'])\n",
    "    last_w = get_vector(mention_features['last_word'])\n",
    "    mention_length = get_vector(mention_features['mention_length'])\n",
    "    mention_type = np.array(mention_features['mention_type']).reshape((4, 1))\n",
    "    mention_position = np.array(mention_features['mention_position']).reshape((1, 1))\n",
    "    \n",
    "    if mention_features['contained'] == False:\n",
    "        mention_contain = np.zeros((1, 1))\n",
    "    else:\n",
    "        mention_contain = np.ones((1, 1))\n",
    "    if len(mention_features['pre_words']) > 0:\n",
    "        mention_p_w1 = get_vector(mention_features['pre_words'][0])\n",
    "    else:\n",
    "        mention_p_w1 = np.zeros((50, 1))\n",
    "    \n",
    "    if len(mention_features['pre_words']) > 1:\n",
    "        mention_p_w2 = get_vector(mention_features['pre_words'][1])\n",
    "    else:\n",
    "        mention_p_w2 = np.zeros((50, 1))\n",
    "    if len(mention_features['next_words']) > 0:\n",
    "        mention_n_w1 = get_vector(mention_features['next_words'][0])\n",
    "    else:\n",
    "        mention_n_w1 = np.zeros((50, 1))\n",
    "    if len(mention_features['next_words']) > 1:\n",
    "        mention_n_w2 = get_vector(mention_features['next_words'][1])\n",
    "    else:\n",
    "        mention_n_w2 = np.zeros((50, 1))\n",
    "    if len(mention_features['pre_words']) > 0:\n",
    "        mention_p_w_a = get_average_vector(mention_features['pre_words'])\n",
    "    else:\n",
    "        mention_p_w_a = np.zeros((50, 1))\n",
    "    if len(mention_features['next_words']) > 0:\n",
    "        mention_n_w_a = get_average_vector(mention_features['next_words'])\n",
    "    else:\n",
    "        mention_n_w_a = np.zeros((50, 1))\n",
    "        \n",
    "        \n",
    "    mention_s_a = get_average_vector(mention_features['mention_sentence'].split())\n",
    "    \n",
    "    doc_avg = get_average_vector(passage)\n",
    "    \n",
    "    features = np.concatenate((first_w, last_w, mention_p_w1, mention_p_w2, mention_p_w_a, \\\n",
    "                              mention_n_w1, mention_n_w2, mention_n_w_a, mention_s_a, mention_length, \\\n",
    "                              mention_type, mention_position, mention_contain, doc_avg))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = get_mention_features_vector(p[0][0], sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pair_features_vector(features):\n",
    "    '''\n",
    "    This function gets all the list of pair features, and returns the vector of the pair features.\n",
    "    \n",
    "    Args:\n",
    "    features: All the features of mentions and pair features.\n",
    "    \n",
    "    Returns:\n",
    "    pair_features: A vector with all the pair features.\n",
    "    '''\n",
    "    # distance features\n",
    "    mention_dist = np.array(features[2]['mention_distance']).reshape((10, 1))\n",
    "    s_dist = np.array(features[3]['sentence_distance']).reshape((10, 1))\n",
    "    overlap = np.array(features[4]['overlap']).reshape((1, 1))\n",
    "    \n",
    "    # speaker feature\n",
    "    speaker = np.array(features[5]['speaker']).reshape((1, 1))\n",
    "    \n",
    "    # string matching features\n",
    "    head_match = np.array(features[6]['head_match']).reshape((1, 1))\n",
    "    mention_exact_match = np.array(features[7]['mention_exact_match']).reshape((1, 1))\n",
    "    mention_partial_match = np.array(features[8]['mention_partial_match']).reshape((1, 1))\n",
    "    \n",
    "    pair_features = np.concatenate((mention_dist, s_dist, overlap, speaker, head_match, \\\n",
    "                                   mention_exact_match, mention_partial_match))\n",
    "    return pair_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_feature_input_vector(feature_list_all_mentions, passage):\n",
    "    '''\n",
    "    This function takes in all the features of mentions(individual features and pair features) and the \n",
    "    passage of the mention, and returns the list of vectors for each mention pair.\n",
    "    '''\n",
    "    \n",
    "    input_feature_list = []\n",
    "    i = 0\n",
    "    for m in feature_list_all_mentions:\n",
    "        input_feature_vector = []\n",
    "        mention_avg = get_average_vector(m[1]['mention'].split())\n",
    "        antecedent_avg = get_average_vector(m[0]['mention'].split())\n",
    "        mention_features = get_mention_features_vector(m[1], passage)\n",
    "        antecedent_features = get_mention_features_vector(m[1], passage)\n",
    "        pair_features = get_pair_features_vector(m)\n",
    "        \n",
    "        input_feature_vector.append(antecedent_avg)\n",
    "        input_feature_vector.append(antecedent_features)\n",
    "        input_feature_vector.append(mention_avg)\n",
    "        input_feature_vector.append(mention_features)\n",
    "        input_feature_vector.append(pair_features)\n",
    "        input_feature_list.append(input_feature_vector)\n",
    "        \n",
    "    return input_feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_input_vector(passage, features):\n",
    "    '''\n",
    "    This function takes in the passage and returns the vector for all the mentions to be\n",
    "    tested for coreference.\n",
    "    \n",
    "    Args:\n",
    "    passage: The passage in which the coreference have to be resolved.\n",
    "    \n",
    "    Returns:\n",
    "    feature_input: The vector which can be fed into the neural network to resolve coreferences.\n",
    "    '''\n",
    "    \n",
    "    #features = get_pair_features(passage)\n",
    "    feature_input = make_feature_input_vector(features, passage)\n",
    "    input_vector_list = []\n",
    "    for f_input in feature_input:\n",
    "        con = np.concatenate((f_input[0], f_input[1], f_input[2], f_input[3], f_input[4]))\n",
    "        input_vector_list.append(con)\n",
    "        del con\n",
    "    return input_vector_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#coref_list = make_input_vector(s5)\n",
    "#len(coref_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1237, 1)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(coref_list[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dev[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = (torch.from_numpy(input_dev[4].reshape(INPUT_DIM))).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = model(Variable(i))\n",
    "_, predicted = torch.max(output.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(int(predicted.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_coref_pairs(passage):\n",
    "    '''\n",
    "    This function takes in the passage for which we want to resolve the coreferences, and returns a list containing \n",
    "    the coreference pairs.\n",
    "    \n",
    "    Args:\n",
    "    passage: The passage/sentence for which we want to resolve the coreferences.\n",
    "    \n",
    "    Returns:\n",
    "    list: A list containing all the possibly resolved coreferences.\n",
    "    '''\n",
    "    coreferences = []\n",
    "    features = get_pair_features(passage)\n",
    "    vector = make_input_vector(passage, features)\n",
    "    print (len(features))\n",
    "    print (len(vector))\n",
    "    for v, f in zip(vector, features):\n",
    "        input_to_NN = (torch.from_numpy(input_dev[1].reshape(INPUT_DIM))).float()\n",
    "        output_from_NN = model(Variable(i))\n",
    "        _, predicted = torch.max(output.data, 0)\n",
    "        if int(predicted.numpy()) == 1:\n",
    "            coreferences.append(({'antecedent': f[0]['mention']}, {'mention': f[1]['mention']}))\n",
    "    return coreferences \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[({'antecedent': 'John'}, {'mention': 'the school'}),\n",
       " ({'antecedent': 'John'}, {'mention': 'He'}),\n",
       " ({'antecedent': 'John'}, {'mention': 'it'}),\n",
       " ({'antecedent': 'the school'}, {'mention': 'He'}),\n",
       " ({'antecedent': 'the school'}, {'mention': 'it'}),\n",
       " ({'antecedent': 'He'}, {'mention': 'it'})]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_coref_pairs('John is in the school. He enjoys it.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
